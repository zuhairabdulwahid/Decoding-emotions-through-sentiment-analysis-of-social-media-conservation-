{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79946840",
   "metadata": {},
   "source": [
    "# Emotion Detection from Social Media Posts\n",
    "\n",
    "This notebook performs sentiment/emotion classification using a pretrained DistilBERT model. You can either use the HuggingFace Emotion dataset or upload your own CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cf1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837daa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Option A - Use HuggingFace Emotion Dataset\n",
    "use_builtin_dataset = True  # Set this to False if uploading your own CSV\n",
    "\n",
    "if use_builtin_dataset:\n",
    "    dataset = load_dataset(\"emotion\")\n",
    "    df = dataset['train'].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54497e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Option B - Upload Your Own CSV Dataset\n",
    "# Run this cell if you want to upload your own file instead of using the HuggingFace dataset\n",
    "\n",
    "if not use_builtin_dataset:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    df = pd.read_csv(list(uploaded.keys())[0])\n",
    "    df = df[['text', 'label']]  # Ensure your CSV has 'text' and 'label' columns\n",
    "    dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize Emotion Distribution\n",
    "sns.countplot(y='label', data=df)\n",
    "plt.title(\"Distribution of Emotions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load Pretrained Model for Classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(set(df['label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe33e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Training Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'] if use_builtin_dataset else encoded_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127453b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Train the Model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Evaluate the Model\n",
    "preds_output = trainer.predict(encoded_dataset['test'] if use_builtin_dataset else encoded_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(preds_output.predictions), axis=1)\n",
    "y_true = torch.tensor(encoded_dataset['test']['label'] if use_builtin_dataset else df['label'])\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Deploy with Gradio\n",
    "import gradio as gr\n",
    "\n",
    "def classify_emotion(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    labels = dataset.features['label'].names if use_builtin_dataset else sorted(df['label'].unique())\n",
    "    return f\"Detected Emotion: {labels[prediction]}\"\n",
    "\n",
    "gr.Interface(fn=classify_emotion, inputs=\"text\", outputs=\"text\", title=\"Emotion Detector from Social Media\").launch()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
